{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "515c7588",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb, os\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "from llama_index.tools.google import GmailToolSpec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f210dc2",
   "metadata": {},
   "source": [
    "#### Function Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b804b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9524a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f614b566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for Medellin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in Medellin is sunny')], tool_name='my_weather_tool', raw_input={'args': ('Medellin',), 'kwargs': {}}, raw_output='The weather in Medellin is sunny', is_error=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_tool_output = function_tool.call(\"Medellin\")\n",
    "function_tool_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fcde70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The weather in Medellin is sunny'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_tool_output.raw_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e47f30f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f3d18",
   "metadata": {},
   "source": [
    "#### Query Engine Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229f291c",
   "metadata": {},
   "source": [
    "Create a **QueryEngineTool** from a **QueryEngine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5b6c682",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(find_dotenv())\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"nlp_papers\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=1000,\n",
    "    token=hf_token,\n",
    "    provider=\"auto\"\n",
    ")\n",
    "query_engine = index.as_query_engine(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d55de951",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tool = QueryEngineTool.from_defaults(query_engine, name=\"my_nlp_research_tool\", \n",
    "                                                  description=\"Useful for getting information about Kevin's NLP research\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "193e36c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text=\"Kevin Martinez's main contribution, as detailed in the provided context, involves developing and evaluating various models for detecting fake news in Spanish. This includes employing classical machine learning techniques such as SVM, Random Forest, Gradient Boosting Tree, and Multi-Layer Perceptron, as well as deep learning methods like LSTM and CNN. The work encompasses preprocessing steps like text normalization, tokenization, stemming, and removal of stopwords, as well as different text representation methods such as Bag of Words, TF-IDF, and Word Embeddings. His research also explores the use of pre-trained embeddings and the application of transfer learning, highlighting the translation of models trained on English datasets to perform effectively on Spanish datasets.\")], tool_name='my_nlp_research_tool', raw_input={'input': 'What is the main contribution of Kevin Martinez to NLP?'}, raw_output=Response(response=\"Kevin Martinez's main contribution, as detailed in the provided context, involves developing and evaluating various models for detecting fake news in Spanish. This includes employing classical machine learning techniques such as SVM, Random Forest, Gradient Boosting Tree, and Multi-Layer Perceptron, as well as deep learning methods like LSTM and CNN. The work encompasses preprocessing steps like text normalization, tokenization, stemming, and removal of stopwords, as well as different text representation methods such as Bag of Words, TF-IDF, and Word Embeddings. His research also explores the use of pre-trained embeddings and the application of transfer learning, highlighting the translation of models trained on English datasets to perform effectively on Spanish datasets.\", source_nodes=[NodeWithScore(node=TextNode(id_='c7da3fe9-da44-4066-9869-ee57e9b1a4aa', embedding=None, metadata={'page_label': '3', 'file_name': 'fake_news_detection_spanish_DL.pdf', 'file_path': 'c:\\\\Users\\\\kevin\\\\Documents\\\\Agents_MCP_dojo\\\\llamaindex\\\\data\\\\fake_news_detection_spanish_DL.pdf', 'file_type': 'application/pdf', 'file_size': 247472, 'creation_date': '2025-09-08', 'last_modified_date': '2025-09-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='b51881e0-a7b5-40eb-be92-7d99bde1a4e7', node_type='4', metadata={'page_label': '3', 'file_name': 'fake_news_detection_spanish_DL.pdf', 'file_path': 'c:\\\\Users\\\\kevin\\\\Documents\\\\Agents_MCP_dojo\\\\llamaindex\\\\data\\\\fake_news_detection_spanish_DL.pdf', 'file_type': 'application/pdf', 'file_size': 247472, 'creation_date': '2025-09-08', 'last_modified_date': '2025-09-08'}, hash='f44390a7ec43c149fcccfa1aa3bbf0547cebe9b0994a80650269561966e23fc0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='3 Methods\\n3.1 Preprocessing steps\\nIn order to obtain consistent results, a data standardization process known as Text Normalization was performed, which,\\nin addition to eliminating non-alphanumeric characters in the text, includes some of the most commonly used techniques\\nin NLP:\\n• Stop Words:we removed words there is an agreement they do not contribute to the models learning process in\\nthe context of the problem addressed; for instance, articles and prepositions.\\n• Stemming: this technique was used to reduce words to their root.\\n• Tokenization and Padding:as usual in text processing tasks, we performed tokenization and padding, when\\nrequired, for words and sentences representation.\\nSubsequently, we decided to compare some of the most common techniques regarding text representation: BoW, which\\nprovides the number of occurrences of each word in the text corpus; term frequency-inverse document frequency(tf-idf),\\nwhich provides a weighted measure of the importance of each term within the text (according to its frequency of\\noccurrence in sentences); and pre-trained Word Embeddings, where words and the semantic relationships among them\\nare represented as a vector. It is worth clarifying that, we call Word Embeddings to both pre-trained vectors such as\\nword2vec or GloVe, and embeddings obtained from pre-trained models such as ELMo or BERT (presented in subsection\\n3.2).\\n3.2 Models\\nClassical ML models, and DL models based on artiﬁcial neural networks were used. We employed ML models intending\\nto create a baseline for comparison purposes; hence, we selected the following: Support Vector Machine (SVM),\\nRandom Forest (RF), Gradient Boosting Tree (GBT), and Multi-Layer Perceptron (MLP). For the case of DL classiﬁers,\\nbesides word embeddings, two types of layers were used: Long Short-Term Memory Recurrent Neural Network\\n(LSTM-RNN) using a many-to-one architecture, and Convolutional Neural Network (CNN). LSTM-RNN processes\\nthe input data as sequences of dependent observations, while CNNs can process n-grams through the application of\\nconvolutional ﬁlters. A schematic of the DL classiﬁers in combination with a embedding layer is illustrated in Figure 1;\\nthis ﬁgure shows the arrangement of the aforementioned layers, and the different word embeddings we used which are\\npresented next.\\nFigure 1: Schematic of DL classiﬁers in combination with Embedding Layer\\n3', mimetype='text/plain', start_char_idx=0, end_char_idx=2382, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.47635674798756056), NodeWithScore(node=TextNode(id_='a6e03e5f-90b9-4634-8c3f-228020d9629e', embedding=None, metadata={'page_label': '6', 'file_name': 'fake_news_detection_spanish_DL.pdf', 'file_path': 'c:\\\\Users\\\\kevin\\\\Documents\\\\Agents_MCP_dojo\\\\llamaindex\\\\data\\\\fake_news_detection_spanish_DL.pdf', 'file_type': 'application/pdf', 'file_size': 247472, 'creation_date': '2025-09-08', 'last_modified_date': '2025-09-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3ab62d6b-fff4-42d2-b6cf-c6a66012bec3', node_type='4', metadata={'page_label': '6', 'file_name': 'fake_news_detection_spanish_DL.pdf', 'file_path': 'c:\\\\Users\\\\kevin\\\\Documents\\\\Agents_MCP_dojo\\\\llamaindex\\\\data\\\\fake_news_detection_spanish_DL.pdf', 'file_type': 'application/pdf', 'file_size': 247472, 'creation_date': '2025-09-08', 'last_modified_date': '2025-09-08'}, hash='c11dbe766b605dbe782c443824743f71236063dfddbed5ee59b530a29feeef6d')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='The source code used to carry out the experiments can be found in a publicly accessible repository at GitHub 3.\\n4.3 Results\\nTable 1 shows the best results for each of the models considered during the experiments of the ﬁrst scheme, which\\nwas described in subsection 4.2. It also shows the conﬁguration of pre-processing steps that achieved the best results.\\nMoreover, the hyperparameter values selected for each model were the following:\\n• (SVM) kernel RBF; ’C’: 1e3; kernel coefﬁcient ’gamma’: 1\\n• (RF and GBT) number of trees: 500; maximum number of features: 50\\n• (MLP) hidden layers: 1; neurons: 10; epochs: 1500\\nTable 1: Baseline results for the dataset in Spanish\\nModel Vocab Size Stemming Remove StopWords Text Representation test_acc\\nSVM 10000 NO YES tf-idf 0.798\\nRF 40000 NO YES tf-idf 0.802\\nGBT 40000 YES NO BoW 0.783\\nMLP 10000 YES NO tf-idf 0.794\\nAccording to the baseline results, RF in combination with a tf-idf text representation showed the highest accuracy.\\nSubsequently, we performed the experiments with the DL models (LSTM, CNN) in combination with the different types\\nof Word Embeddings; hence, we followed the second, third, and fourth schemes. From this point on, we permanently\\nremoved Stop Words and did not apply Stemming anymore regarding data pre-processing.\\nInitially, we ran some experiments using a trainable embedding layer; the results are summarized in Table 2, where the\\nhyperparameter values selected for each model were:\\n• LSTM (Spanish) 16 units; KR and KK equals 1; D equals 0\\n• LSTM (English) 4 units; KR and KK equals 0.01; D equals 0\\n• CNN (Spanish) F equals 16; KS equals 10; 4 units; KR equals 0.01\\n• CNN (English) F equals 16; KS equals 10; 12 units; KR equals 0\\nThe LSTM and CNN models trained with the English dataset, whose results are shown in Table 2, were also validated\\nwith the whole translated dataset, yielding accuracies of 56.7% and 53.2% respectively.\\nTable 2: Results for DL models with trainable embedding layer; the column dev_acc shows the accuracy in the\\ndevelopment set; std is the standard deviation, and test_acc shows the accuracy in the test set.\\nModel Dataset Language dev_acc std test_acc\\nLSTM Spanish 0.714 0.026 0.761\\nLSTM English 0.95 0.02 0.931\\nCNN Spanish 0.73 0.021 0.685\\nCNN English 0.984 0.002 0.982\\nNext, we performed the experiments using a Transfer Learning approach, with the pre-trained 300-feature GloVe\\nembedding layer presented in subsection 3.2; this time, the embedding values were left ﬁxed during the training process,\\nand only the added hidden layers were ﬁne-tuned. Since the GloVe vectors utilized were trained on a corpus in English,\\nthese experiments correspond to the third and fourth schemes. The results are summarized in Table 3; moreover, the\\nhyperparameter values chosen for each model were the following:\\n• (LSTM) 8 units; KR and KK equals 0; D equals 0.5\\n• (CNN) F equals 16; KS equals 10; 4 units; KR equals 0\\nThen, when validating the former models using the translated dataset (fourth scheme), we got accuracy values of 54%\\nand 53.8% for LSTM and CNN layers, respectively.\\n3https://github.com/kevinmaiden7/Spanish_FakeNewsDetection\\n6', mimetype='text/plain', start_char_idx=0, end_char_idx=3138, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.46875538337781664)], metadata={'c7da3fe9-da44-4066-9869-ee57e9b1a4aa': {'page_label': '3', 'file_name': 'fake_news_detection_spanish_DL.pdf', 'file_path': 'c:\\\\Users\\\\kevin\\\\Documents\\\\Agents_MCP_dojo\\\\llamaindex\\\\data\\\\fake_news_detection_spanish_DL.pdf', 'file_type': 'application/pdf', 'file_size': 247472, 'creation_date': '2025-09-08', 'last_modified_date': '2025-09-08'}, 'a6e03e5f-90b9-4634-8c3f-228020d9629e': {'page_label': '6', 'file_name': 'fake_news_detection_spanish_DL.pdf', 'file_path': 'c:\\\\Users\\\\kevin\\\\Documents\\\\Agents_MCP_dojo\\\\llamaindex\\\\data\\\\fake_news_detection_spanish_DL.pdf', 'file_type': 'application/pdf', 'file_size': 247472, 'creation_date': '2025-09-08', 'last_modified_date': '2025-09-08'}}), is_error=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_tool_outut = query_engine_tool.call(\"What is the main contribution of Kevin Martinez to NLP?\")\n",
    "query_engine_tool_outut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e4d60f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kevin Martinez's main contribution, as detailed in the provided context, involves developing and evaluating various models for detecting fake news in Spanish. This includes employing classical machine learning techniques such as SVM, Random Forest, Gradient Boosting Tree, and Multi-Layer Perceptron, as well as deep learning methods like LSTM and CNN. The work encompasses preprocessing steps like text normalization, tokenization, stemming, and removal of stopwords, as well as different text representation methods such as Bag of Words, TF-IDF, and Word Embeddings. His research also explores the use of pre-trained embeddings and the application of transfer learning, highlighting the translation of models trained on English datasets to perform effectively on Spanish datasets.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_engine_tool_outut.raw_output.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8169284",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496544db",
   "metadata": {},
   "source": [
    "#### Toolspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d117d95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x293b97a75b0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x293b97968f0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x293b897dcd0>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x293b897dd90>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x293b87e2150>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x293b87e2360>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56af2313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool: load_data | Description: load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "\n",
      "Tool: search_messages | Description: search_messages(query: str, max_results: Optional[int] = None)\n",
      "Searches email messages given a query string and the maximum number\n",
      ":param query: The user's query\n",
      ":param max_results: The maximum number of search results\n",
      "\n",
      "Tool: create_draft | Description: create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "Create and insert a draft email.\n",
      ":param to: The email addresses to send the message to\n",
      ":param subject: The subject for the event\n",
      ":param message: The message for the event\n",
      "\n",
      "Tool: update_draft | Description: update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "Update a draft email.\n",
      ":param to: The email addresses to send the message to\n",
      ":param subject: The subject for the event\n",
      ":param message: The message for the event\n",
      ":param draft_id: the id of the draft to be updated\n",
      "\n",
      "Tool: get_draft | Description: get_draft(draft_id: str = None) -> str\n",
      "Get a draft email.\n",
      ":param draft_id: the id of the draft to be updated\n",
      "\n",
      "Tool: send_draft | Description: send_draft(draft_id: str = None) -> str\n",
      "Sends a draft email.\n",
      ":param draft_id: the id of the draft to be updated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tool in tool_spec_list:\n",
    "    print(f\"Tool: {tool.metadata.name} | Description: {tool.metadata.description}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents_mcp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
